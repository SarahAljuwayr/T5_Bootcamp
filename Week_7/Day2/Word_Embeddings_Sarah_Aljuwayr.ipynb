{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as continuous vectors in a high-dimensional space. Unlike traditional representations like Bag of Words (BoW), word embeddings capture semantic meanings and relationships between words by placing similar words closer together in the vector space.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Word Embedding**: A dense vector representation of a word where each dimension captures some aspect of its meaning.\n",
        "2. **Pre-trained Embeddings**: Embeddings learned from large corpora, such as Word2Vec, GloVe, and FastText.\n",
        "3. **Semantic Similarity**: Words with similar meanings will have similar embeddings, making it easier to perform tasks like word similarity and analogy."
      ],
      "metadata": {
        "id": "ihBopOobLb2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "J14EKzDQLg2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load pre-trained Word2Vec model (Google News vectors)\n",
        "Note: This model is quite large. For demonstration, use a smaller or different model as needed.\n",
        " model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "6YBv0QyqLiSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, we'll use a smaller pre-trained model available in gensim\n",
        "from gensim.downloader import load\n",
        "model = load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "Iyp8HlpbLmHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "words = ['king', 'queen', 'man', 'woman']"
      ],
      "metadata": {
        "id": "xkigRBgHLoiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "embeddings = {word: model[word] for word in words}"
      ],
      "metadata": {
        "id": "xJngiE2qLqOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H01quYFFLWE0"
      },
      "outputs": [],
      "source": [
        "# Display embeddings\n",
        "for word, vector in embeddings.items():\n",
        "    print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.most_similar('computer', topn=5)\n",
        "\n",
        "# TODO:: Display similar words"
      ],
      "metadata": {
        "id": "rKCazO0NLsOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve analogy\n",
        "analogy_result = model.most_similar(positive=['queen', 'man'], negative=['king'], topn=1)\n",
        "\n",
        "# TODO:: Display result"
      ],
      "metadata": {
        "id": "fkcpJPKvL43C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "h1f6oM0h0ROd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.downloader import load\n",
        "model = load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "m6CaZ-xk0hyj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['prince', 'princess', 'boy', 'girl']"
      ],
      "metadata": {
        "id": "xcg1SOrG0tXw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {word: model[word] for word in words}"
      ],
      "metadata": {
        "id": "57e5MHb-0_QM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, vector in embeddings.items():\n",
        "  print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF5LT1pS1KZR",
        "outputId": "60e7c275-a713-4520-ba83-151b693a51f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: prince\n",
            "Embedding: [ 9.8846e-01  1.4535e+00 -5.3081e-01  1.0509e-01  8.4058e-01  1.4018e-01\n",
            "  6.6562e-02  1.3341e+00 -7.5813e-01 -3.5223e-01  1.6588e-01  1.0016e+00\n",
            "  1.9623e-02 -6.6392e-01  9.2825e-02  2.5132e-01 -1.6274e-01 -1.1954e-01\n",
            " -5.0072e-01  6.7374e-01  6.6886e-01 -3.8679e-02  2.0223e-01 -1.5211e-01\n",
            "  1.2169e-01 -1.8324e+00 -8.5664e-01 -6.2454e-01 -3.1896e-01  6.0221e-01\n",
            "  1.4110e+00  5.0157e-01 -1.1413e-01  5.1808e-01  8.4700e-01  1.7618e-01\n",
            " -3.5265e-02  5.6405e-01 -3.8524e-01  6.0270e-01  3.4331e-01  1.1836e+00\n",
            " -3.7197e-01 -1.1069e+00  1.2758e-04 -1.8202e-01 -1.3696e+00 -1.4970e+00\n",
            "  4.0618e-01 -4.2445e-01]\n",
            "\n",
            "Word: princess\n",
            "Embedding: [ 1.4992    1.6053   -1.1699    0.69597   0.63491   1.0803   -0.15271\n",
            "  1.0974   -0.12842  -0.74608   0.59572   0.76493   0.18664  -0.47217\n",
            "  0.72322   0.48368  -1.4334   -0.032644 -0.1652    0.38196   0.71329\n",
            "  0.89524  -0.26091   0.44074   0.52343  -1.2422   -1.8396   -0.10232\n",
            " -0.081415 -0.087913  0.91303   0.69385   0.26242   1.0921    0.66015\n",
            " -0.19193  -0.028172 -0.13914  -0.54633  -0.75658   0.45368   0.066445\n",
            "  0.49774  -1.2702    0.10915  -0.46564  -0.65682  -2.0678    0.72255\n",
            " -0.35506 ]\n",
            "\n",
            "Word: boy\n",
            "Embedding: [-0.32345   0.23332  -0.20082  -0.52848   1.0926    0.62445  -0.99859\n",
            "  0.28085   0.088326  0.36919   0.32199   0.3499    0.067459  0.24211\n",
            "  0.92565  -0.32581  -0.99134   0.80767  -0.22845   0.40076  -0.8577\n",
            "  1.3836    0.056439  0.76561   0.3608   -2.0692   -0.46679   0.12359\n",
            "  0.35127  -0.77092   2.2064   -0.42605  -0.24279   0.3832    0.6069\n",
            "  0.62835   0.31825  -0.8851    0.38329  -1.146    -0.41949   0.2606\n",
            " -0.6568   -0.11511   1.0591   -0.61148   0.32152  -1.3182    0.31744\n",
            "  0.02527 ]\n",
            "\n",
            "Word: girl\n",
            "Embedding: [-0.34471   0.69563  -0.78086  -0.58482   1.2263    1.2544   -0.76466\n",
            "  0.40575   0.18862   0.098834  0.32557  -0.31816   0.23869   0.33554\n",
            "  1.0592   -0.25266  -1.0308    0.70027   0.030457  0.53866  -0.30279\n",
            "  1.7515    0.3128    1.2103    0.41335  -1.9421   -0.93756   0.32453\n",
            "  0.52249  -0.86708   2.1258   -0.20377  -0.19061   0.37736   0.66205\n",
            "  0.50211  -0.12544  -0.83069   0.21155  -1.3091   -0.44973  -0.11648\n",
            "  0.033598 -0.68382   0.99419  -0.88262   0.56601  -1.327     0.37319\n",
            "  0.022389]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.most_similar('pen', topn =5)\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka7x5GdK1dD4",
        "outputId": "ad4c5519-fd02-4381-9263-b8f632d4d074"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ballpoint', 0.6782395839691162),\n",
              " ('pencil', 0.6529707908630371),\n",
              " ('pens', 0.6143139004707336),\n",
              " ('duick', 0.597665548324585),\n",
              " ('paper', 0.588976263999939)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy_result = model.most_similar(positive=['princess','boy'], negative=['prince'], topn=1)\n",
        "analogy_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-cTIML91wmP",
        "outputId": "0a327e02-6af9-4e27-bbf7-32341cba327e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.8908403515815735)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}